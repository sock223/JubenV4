Ngram(也称为N元模型)是自然语言处理中一个非常重要的概念。在NLP中，人们基于一定的语料库，可以利用Ngram来预计或者评估一个句子是否合理。另外一方面，Ngram可以用来评估两个字符串之间的差异程度，这是模糊匹配中常用的一种手段。而且广泛应用于机器翻译、语音识别、印刷体和手写体识别、拼写纠错、汉字输入和文献查询。

0 Ngram Model
假定S表示某个有意义的句子，由一串特定顺序排列的词w1,w2,w3,..,wn组成，n是句子的长度。想知道S在文本中(语料库)出现的可能性，也就是数学上所说的概率P(S)
可是这样的方法存在两个致命的缺陷：
參数空间过大：条件概率P(wn|w1,w2,..,wn-1)的可能性太多，无法估算，不可能有用；
数据稀疏严重：对于非常多词对的组合，在语料库中都没有出现，依据较大似然估计得到的概率将会是0。最后的结果是，我们的模型仅仅能算可怜兮兮的几个句子，而大部分的句子算得的概率是0。
1 马尔科夫假设
为了解决參数空间过大的问题。引入了马尔科夫假设：随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关。
如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram
假设一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram
一般来说，N元模型就是假设当前词的出现概率只与它前面的N-1个词有关。而这些概率参数都是可以通过大规模语料库来计算，比如三元概率.
在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多。

2 数据平滑
有研究人员用150万词的训练语料来训练trigram模型，然后用同样来源的测试语料来做验证，结果发现23%的trigram没有在训练语料中出现过。对语言而言，由于数据稀疏的存在，极大似然法不是一种很好的参数估计办法。这时的解决办法，我们称之为“平滑技术”。(參考《数学之美》第33页)
数据平滑的目的有两个：一个是使全部的Ngram概率之和为1；一个是使全部的Ngram概率都不为0。其主要策略是把在训练样本中出现过的事件的概率适当减小，然后把减小得到的概率密度分配给训练语料中没有出现过的事件。实际中平滑算法有很多种，例如：
Add-one平滑
Witten-Bell平滑
Good-Turing平滑
Katz Backoff
Stupid Backoff
数据平滑技术是构造高鲁棒性语言模型的重要手段，且数据平滑的效果与训练语料库的规模有关。训练语料库规模越小，数据平滑的效果越显著；训练语料库规模越大，数据平滑的效果越不显著，甚至可以忽略。
3 基于Ngram模型定义的字符串距离
模糊匹配的关键在于如何衡量两个长得很像的单词(或字符串)之间的“差异”，这种差异通常又称为“距离”。除了可以定义两个字符串之间的编辑距离(通常利用Needleman-Wunsch算法或Smith-Waterman算法)，还可以定义它们之间的Ngram距离。
假设有一个字符串S，那么该字符串的Ngram就表示按长度N切分原词得到的词段，也就是S中所有长度为N的子字符串。设想如果有两个字符串，然后分别求它们的Ngram，那么就可以从它们的共有子串的数量这个角度去定义两个字符串间的Ngram距离。但是仅仅是简单地对共有子串进行计数显然也存在不足，这种方案显然忽略了两个字符串长度差异可能导致的问题。比如字符串girl和girlfriend，二者所拥有的公共子串数量显然与girl和其自身所拥有的公共子串数量相等，但是我们并不能据此认为girl和girlfriend是两个等同的匹配。为了解决该问题，有学者提出以非重复的Ngram分词为基础来定义Ngram距离
显然，字符串之间的距离越小，它们就越接近。当两个字符串完全相等的时候，它们之间的距离就是0。


4 利用Ngram模型评估语句是否合理
从统计的角度来看，自然语言中的一个句子S可以由任何词串构成，不过概率P(S)有大有小。例如：
S1 = 我刚吃过晚饭
S2 = 刚我过晚饭吃
显然，对于中文而言S1是一个通顺而有意义的句子，而S2则不是，所以对于中文来说P(S1)>P(S2)。
另外一个例子是，如果我们给出了某个句子的一个节选，我们其实可以能够猜测后续的词应该是什么，例如：
the large green __ . mountain or tree ?
Kate swallowed the large green __ . pill or broccoli ?
5 基于Ngram模型的文本分类器
Ngram如何用作文本分类器的呢？其实很简单了，只要根据每个类别的语料库训练各自的语言模型，实质上就是每一个类别都有一个概率分布，当新来一个文本的时候，只要根据各自的语言模型，计算出每个语言模型下这篇文本的发生概率，文本在哪个模型的概率大，这篇文本就属于哪个类别了！
6 Ngram在语言识别中的应用
Ngram是从文本或文档中提取的字符或单词序列，可被分成两组：基于字符的或基于单词的。一个Ngram是提取自一个单词（在我们例子中是一个字符串）的一组N个连续字符。其后的动机是类似的单词将具有高比例的Ngram。最常见的N值是2和3，分别称为bigram和trigram。比如，单词TIKA生成的bigram为*T、TI、IK、KA、A*，生成的trigram为**T、*TI、TIK、IKA、KA*、A**。*代表的是一个补充空间。基于字符的Ngram被用来量度字符串的相似性。使用基于字符的Ngram有些应用程序有拼写检查程序、stemming和OCR。
单词Ngram是提取自文本的N个连续单词的序列。它也独立于语言。基于两个字符串间的相似性的Ngram是由Dice的系数衡量的。s = (2|X /\ Y|)/(|X| + |Y|)，其中X和Y是这个字符集。/\表示两个集间的一个交集。
语言的识别是怎样实现的？总的来说，当要判断一个新的文档是用的什么语言时，我们首先要创建文档的Ngram概要文件并算出这个新文档概要文件与语言概要文件之间的距离。这个距离的计算根据的是两个概要文件之间的“out-of-place measure”。选择最短的距离，它表示此特定的文档属于该语言。这里要引入一个阈值，它的作用是当出现任何超过阈值的距离时，系统就会报告这个文档的语言不能被判定或判定有误。


7 ARPA格式的Ngram语言模型


8 Ngram语言模型训练工具SRILM
SRILM的主要目标是支持语言模型的估计和评测。估计是从训练数据中得到一个模型，包括较大似然估计及相应的平滑算法；而评测则是从测试集中计算其困惑度。其最基础和最核心的模块是Ngram模块，包括两个工具：ngram-count和ngram，相应的被用来估计语言模型和计算语言模型的困惑度。




ft.CountVectorizer(ngram_range=(1,2))
比如 ABCDE 拆成    A  B AB  C BC   D   E  DE   不仅仅是一个词了 有的词还跟他上一个相关 去算词频，加入了词组对预测的影响

预测语言的合理性，鸡蛋前面有吃 有900次
鸡蛋后面有吃有2词
那么p1 = 900/总数
p2 = 2/总数
p1>p2 p1更合理


研究的是样本的一个词，只与前几个词有关
多词关联性越强 ，越有效果

词对排列组合表达一个意思的可能性太多  ->马尔科夫假设，只与前面的几个单词有关
样本稀疏，很多次的排列组合，资料库中没有 ->数据平滑


探究的是，一个词出现在另一个词前面的概率

比如 样本中有很多中国瓷器，但是很少有瓷器中国，那么中国瓷器的概率大 更像人话

如果 中国瓷器
打中国 将样本2gram后 中国瓷器出现的概率高，那么提示是否输入瓷器两个字
